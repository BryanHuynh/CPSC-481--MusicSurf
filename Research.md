[Home](https://colinauyeung.github.io/CPSC-481--MusicSurf/) - [Stage1](https://colinauyeung.github.io/CPSC-481--MusicSurf/Stage1) - [Stage2](https://colinauyeung.github.io/CPSC-481--MusicSurf/Stage2) - [Stage3](https://colinauyeung.github.io/CPSC-481--MusicSurf/Stage3) - [Stage4](https://colinauyeung.github.io/CPSC-481--MusicSurf/Stage4) - [Stage5](https://colinauyeung.github.io/CPSC-481--MusicSurf/Stage5)  
## Secondary Research
### Findings 
A workshop done by White et al. [9] on user engaged search system offers some general implications for exploratory search interfaces. One such important implication is the difficulty of testing such a system due to long term use and evolution of how a user will approach the system.

Work such as MIL [10], Playsom [7], Schedl et al’s deepTune [8] and that done by Muelder et al [6]. show that automatic grouping systems for music using machine learning and similarity calculations are feasible and can produce human understandable clusters to aid user exploration of music as well as the variety of heuristic approaches that are possible.

On the other hand, work such as MuVis [3] and MusicalNodes [2] show the effectiveness of graphical interfaces for delivery of musical libraries. Both found their systems improved speed of navigation over traditional systems as well saw generally positive reception to a more visual model of display music libraries.  

Lee et al.’s [5] work on understanding how users define mood in music provides insights into how complicated users think mood in music is. Fujino et al. [4] and Moodplay [1] look to deal with the issues of mood for song recommendation with the former focusing on user feedback to define mood and the latter using visualization.


### References 

[1] Andjelkovic, I., Parra, D., & Odonovan, J. (2016). Moodplay. Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization - UMAP 16. doi: 10.1145/2930238.2930280

[2] Dalhuijsen, L., & van Velthoven , L. (2010). MusicalNodes – the visual music library . Proceedings of the 2010 International Conference on Electronic Visualisation and the Arts, 232–236.

[3] Dias, R. J., & Fonseca, M. (2010). MuVis: An application for interactive exploration of large music collections. MM'10 - Proceedings of the ACM Multimedia 2010 International Conference, 1043-1046.

[4] Fujino, H., Hasida, K., & Matsubara, Y. (2017). Music exploration by impression based interaction. Proceedings of the 2017 ACM Workshop on Exploratory Search and Interactive Data Analytics, 5-58. doi: 10.1145/3038462.3038468

[5] Lee, J. H., Hill, T., & Work, L. (2012). What does music mood mean for real users? Proceedings of the 2012 IConference on, 112-119. doi: 10.1145/2132176.2132191

[6] Muelder, C., Provan, T., & Ma, K.-L. (2010). Content based graph visualization of audio data for music library navigation. 2010 IEEE International Symposium on Multimedia, 129-136. doi: 10.1109/ism.2010.27

[7] Neumayer, R., Dittenbach, M., & Rauber, A. (2005). Playsom and Pocketsomplayer, alternative interfaces to large music collections. International Society for Music Information Retrieval.

[8] Schedl, M., Höglinger, C., & Knees, P. (2011). Large-scale music exploration in hierarchically organized landscapes using prototypicality information. Proceedings of the 1st ACM International Conference on Multimedia Retrieval, 1-7. doi: 10.1145/1991996.1992004

[9] White, R., Drucker, S., Marchionini, G., Hearst, M., & Schraefel, M. (2007). Exploratory search and HCI: Designing and evaluating interfaces to support exploratory search interaction. CHI '07 Extended Abstracts on Human Factors in Computing Systems, 2877-2880. doi: 10.1145/1240866.1241100

[10] Wu, X., Qiao, Y., & Tang, X. (2015). MIL: Music Exploration and Visualization via Lyric and Image. Proceedings of the 23rd ACM International Conference on Multimedia, 1011-1014.
